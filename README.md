# C minus minus

A compiler for a C-like programming language, with a reduced set of instructions.

## Table of Contents

- [Summary](#summary)
- [Motivation and Problem to Solve](#motivation-and-problem-to-solve)
- [Project Objectives](#project-objectives)
- [State of the Art](#state-of-the-art)
- [Compiler Architecture and Design](#compiler-architecture-and-design)
  - [Block Diagram](#block-diagram)
  - [Data Flow Explanation](#data-flow-explanation)
  - [Design Decisions](#design-decisions)
- [Lexical Analysis](#lexical-analysis)
  - [Automata](#automata)
- [Syntax Analysis](#syntax-analysis)
- [Semantic Analysis](#semantic-analysis)
- [Installation and Running](#installation-and-running)
- [Testing and Validation](#testing-and-validation)
- [Tools and Development Environment](#tools-and-development-environment)
- [Demonstration](#demonstration)
- [Challenges and Solutions](#challenges-and-solutions)
- [Conclusions and Future Work](#conclusions-and-future-work)
- [References](#references)

## Summary

This project aims to develop a complete compiler, covering everything from lexical analysis to final code generation. The compiler will allow for the translation of source code written in a C-- into machine code.

## Motivation and Problem to Solve

- **Problem Description:** We aim to expand our knowledge by creating a compiler, as building one is a powerful way to gain a deep understanding of how compilers work. And what better way to do this than by creating a compiler for a C-like language?
- **Importance:** An efficient compiler is essential for any development environment, as it directly impacts developer productivity and the quality of the final product. Moreover, having a custom compiler allows for greater customization and optimization for specific use cases.
- **Use Cases:** This compiler will be useful in both academic and industrial settings, enabling the compilation of C++ code and its adaptation to various architectures and platforms. It will also serve as a teaching and experimentation tool in compiler courses.

## Project Objectives

- Develop a lexical analyzer that correctly identifies and classifies the lexical units of the input language.
  -Implement a syntax analyzer that constructs abstract syntax trees (AST) from the tokens generated by the lexical analyzer.
- Create a semantic analyzer that checks the coherence and correctness of the source code in terms of data types, variable scope, and language rules.
- Design a code generator that translates the AST into machine code or an intermediate code optimized for efficient execution.
- Include additional optimizations in the code generation to improve the efficiency of the compiled code.
- Develop an error handler that identifies, reports, and suggests solutions to lexical, syntactic, and semantic errors.

## State of the art

- **Similar Compilers:** GCC, Clang, and other compilers that support C/C++ and provide advanced analysis, code optimization, and machine code generation.
- **Limitations of Current Solutions:** Many current compilers can be difficult to adapt or customize for specific needs. They can also be complex and hard to learn for those just starting in compiler development.
- **Justification for the New Compiler:** This project aims to create a modular and extensible compiler that is not only efficient but also easy to understand and modify. It focuses on a structure that allows for the incorporation of new features and optimizations without needing to restructure the entire compiler.

## Compiler Architecture and Design

- **Block Diagram:**
  ![image](https://github.com/user-attachments/assets/f55bb4f8-9be9-4076-a452-e2bbc98ba996)
  (Figure 1.1, Cooper & Torczon, 2011, p. 9)

- **Data Flow Explanation:**

  1. **Front End:**
     The front end of the compiler is responsible for analyzing the source code and converting it into an intermediate form.

     - **Scanner:**  
       The scanner (also known as the lexical analyzer) scans the source code and divides it into tokens (smallest units like keywords, identifiers, and operators). It also removes any unnecessary whitespaces or comments.

     - **Parser:**  
       The parser checks the syntax of the token stream produced by the scanner. It builds a **parse tree** or **abstract syntax tree (AST)** based on the grammar of the source language. This ensures the code follows the proper syntax rules.

     - **Elaboration:**  
       This phase performs semantic checks (such as type checking) to ensure that the parsed structure makes sense in terms of meaning. It verifies correct variable declarations, function calls, and ensures that expressions are logically valid. The elaboration phase is critical for ensuring that the code is semantically correct before it proceeds to optimization.

  2. **Optimizer:**

     - The optimizer takes the intermediate representation from the front end and attempts to improve the code for efficiency without changing its functionality.

  3. **Back End:**

     - The back end is responsible for generating the target machine code from the optimized intermediate representation.

     - **Instruction Selection:**  
       In this phase, the compiler selects appropriate machine instructions to implement the operations in the intermediate code. This step is specific to the target architecture.

     - **Instruction Scheduling:**  
       This phase reorders the machine instructions to improve performance by avoiding pipeline stalls and ensuring better use of the processor's execution units.

     - **Register Allocation:**  
       In this phase, variables and temporary values are assigned to the limited number of CPU registers. The register allocator may also insert instructions to load and store values from memory when the registers are full.

  4. **Infrastructure:**
     - **Infrastructure:**  
       This layer supports the entire compilation process by providing necessary data structures, algorithms, and management of symbol tables, error handling, and other utilities. The infrastructure ensures that each phase has access to shared resources like intermediate representations and data structures.

- **Design Decisions:**

  1. **Modular Architecture:**

     - The architecture is divided into three key phases—**Front End**, **Optimizer**, and **Back End**—making the design modular and allowing for the independent development and optimization and testing of each phase. This also allows flexibility for porting the compiler to different architectures by adjusting only the back end.

  2. **Separation of Concerns:**

     - The **Front End** focuses on source code analysis and validation, the **Optimizer** improves the performance, and the **Back End** generates machine-specific code. This clear division of responsibilities simplifies maintenance and allows for enhancements without affecting other stages.

  3. **Target-specific Code Generation:**

     - The **Back End** is highly architecture-specific, especially during **Instruction Selection**, **Instruction Scheduling**, and **Register Allocation**, allowing the compiler to produce efficient machine code tailored to the target hardware.

  4. **Infrastructure Support:**
     - A robust **Infrastructure** layer supports all phases of the compiler. It provides necessary services like memory management, symbol table handling, and error reporting, enabling smooth communication and resource sharing among different phases.

## Lexical Analysis

### Lexical Analysis

- Tokenization, identification of keywords, operators, etc.
- Keywords, tokens, identifiers, etc., are the same as those in the C language.

### Automata

Both of these DFAs are the same but were broken down into two images for better viewing clarity.

![Automata Compilador identifier, keyword](https://github.com/user-attachments/assets/695c0dce-311f-41f3-afb7-6a2bc6482d45)
![Automata Compilador operator, constant, punctiation](https://github.com/user-attachments/assets/335a3b7c-0970-485b-887f-e8e05a3d3649)

We implemented a **table-driven lexer** based on the automata diagram. This approach allows for a flexible and generalized solution. The table-driven lexer was designed manually (not generated by any tool), ensuring that it meets the specific requirements and complexities of the language being compiled.

A table-driven lexer provides several advantages over a directly coded lexer:

- **Modularity and Flexibility:** Since the lexer’s logic is abstracted into tables (often a state-transition table), the DFA can be easily modified or extended by changing the table entries without altering the core logic of the lexer.
- **Maintainability:** The table-driven approach simplifies maintenance. If new keywords, tokens, or operators need to be added, it can be done by updating the relevant table entries.
- **Efficiency:** This method can be computationally efficient as it avoids large, nested conditional structures (such as if-else or switch cases) by making use of direct table lookups.

## Syntax Analysis

- **Syntax analysis:**

  - Use of grammars and syntax trees.

- **Examples:**

## Semantic Analysis

- **Semantic analysis:**

  - Use of grammars and syntax trees.

- **Examples:**

## Installation and Running

### Prerequisites

If you don't have Conan installed, run the following:

```bash
pip install conan
conan profile detect
```

### Install Conan packages (gtest)

From the root of the project directory, run the following command to install Conan packages:

```bash
conan install . --build=missing --output-folder=build -s build_type=Debug
```

### Build the Project

To build the project, run the following:

```bash
cd build/
cmake ..
cmake --build .
```

### Build and Run the Project

To build and run the project, execute:

```bash
cd build/
cmake --build . --target run
```

## Testing and Validation

- **Testing methodology:**
- **Results obtained:**
- **Specific test cases:**

## Tools and Development Environment

- **Programming languages used:**
- **Development tools:**
- **Testing and simulation environment:**

## Demonstration

- **Source code example:**
- **Compilation process:**
- **Execution of compiled code:**

## Challenges and Solutions

- **Technical or design problems:**
- **Strategies adopted to overcome challenges:**
- **Lessons learned:**

## Conclusions and Future Work

- **Summary of objectives achieved:**
- **Performance evaluation:**
- **Proposals for future improvements:**

## References

- Cooper, K., & Torczon, L. (2011). Engineering a compiler (2nd ed.). Morgan Kaufmann.
